{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d91617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From g:\\Softwares\\Miniconda\\envs\\convo_capsule\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Libraries imported successfully!\n",
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, T5Tokenizer, TFT5ForConditionalGeneration\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8e759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample transcript loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Our Sample Data (Input)\n",
    "# (Please re-run this cell to ensure we're using this text)\n",
    "\n",
    "MEETING_TRANSCRIPT = \"\"\"\n",
    "Tom: Okay everyone, let's kick off. The main goal today is to finalize the new marketing slogan for the Q4 launch. Sarah, what does your team have?\n",
    "\n",
    "Sarah: Thanks, Tom. We've narrowed it down to three options. \"Innovation for Tomorrow,\" \"Your Future, Our Passion,\" and \"Simply Better.\" The data suggests \"Simply Better\" is resonating most with our test groups.\n",
    "\n",
    "Alex: I agree. It's clean and direct. \"Innovation for Tomorrow\" is too generic.\n",
    "\n",
    "Tom: Good point, Alex. Let's go with \"Simply Better.\" Sarah, can you please get the final design assets to the web team?\n",
    "\n",
    "Sarah: Will do. I'll have them sent over by end-of-day Friday.\n",
    "\n",
    "Alex: I also have an action item. I will coordinate with the legal team to get the trademark paperwork started for \"Simply Better.\" I should have an update on that by our next meeting.\n",
    "\n",
    "Tom: Perfect. That's all for today. Great work, team.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample transcript loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55cb0f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading summarization model... (This may take a moment on first run)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to torch.float32 because loading with the original dtype failed on the target device.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ✅ MEETING SUMMARY ---\n",
      "\"Simply Better\" is resonating most with our test groups,\" says Tom. \"Innovation for Tomorrow\" is too generic. Sarah, can you please get the final design assets to the web team?\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading summarization model... (This may take a moment on first run)\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "summary_output = summarizer(\n",
    "    MEETING_TRANSCRIPT, \n",
    "    max_length=90, \n",
    "    min_length=30, \n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(\"\\n--- ✅ MEETING SUMMARY ---\")\n",
    "print(summary_output[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae126793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading FLAN-T5 model with a new prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From g:\\Softwares\\Miniconda\\envs\\convo_capsule\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "g:\\Softwares\\Miniconda\\envs\\convo_capsule\\lib\\site-packages\\tf_keras\\src\\initializers\\initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ✅ ACTION ITEMS ---\n",
      "Tom, Sarah, Alex, Alex and Tom will finalize the new slogan for the Q4 launch. Sarah will have the final design assets sent over by end-of-day Friday. Alex will coordinate with the legal team to get the trademark paperwork started for \"Simply Better.\"\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Task 2 - Extract Action Items (Using a Simple Prompt)\n",
    "\n",
    "print(\"\\nLoading FLAN-T5 model with a new prompt...\")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = TFT5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", from_pt=True)\n",
    "\n",
    "# --- THIS IS THE NEW, SIMPLIFIED PROMPT ---\n",
    "# We are removing the complex \"Format as...\" instructions\n",
    "# and just giving a clear task.\n",
    "prompt = f\"\"\"\n",
    "What are the assigned tasks for this transcript.\n",
    "\n",
    "Transcript:\n",
    "{MEETING_TRANSCRIPT}\n",
    "\n",
    "Assigned Tasks:\n",
    "\"\"\"\n",
    "\n",
    "# Now we run the model\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"tf\", max_length=1024, truncation=True)\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs, \n",
    "    max_length=200, \n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "action_items_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- ✅ ACTION ITEMS ---\")\n",
    "print(action_items_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf3128bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio file: A1-044-LYRA-WHERE-DO-YOU-GO-IN-THE-MORNING.mp3...\n",
      "Audio loaded and resampled to 16kHz successfully.\n",
      "\n",
      "Loading Whisper ASR model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing audio... (This may take a moment)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ✅ TRANSCRIBED TEXT ---\n",
      " Hello, my name is Lura. I am from Kosovo. My question is, where do you go in the morning? I don't go anywhere in the morning. I like to stay home. In the morning, I always sleep. I am not a morning person. I am a night person. I wake up very late. I love to stay home in the morning. I like my mornings, peaceful and quiet. What about you? Where do you go in the morning?\n",
      "\n",
      "Loading summarization model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ✅ FINAL SUMMARY (FROM AUDIO) ---\n",
      "Lura, from Kosovo, says she is not a morning person. \"I like my mornings, peaceful and quiet,\" she says. Where do you go in the morning?\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Full Pipeline (Audio -> Text -> Summary) - Corrected for Long Audio\n",
    "\n",
    "from transformers import pipeline\n",
    "import librosa\n",
    "import tensorflow as tf \n",
    "\n",
    "AUDIO_FILE_PATH = \"A1-044-LYRA-WHERE-DO-YOU-GO-IN-THE-MORNING.mp3\" \n",
    "\n",
    "print(f\"Loading audio file: {AUDIO_FILE_PATH}...\")\n",
    "\n",
    "try:\n",
    "    input_audio_array, sample_rate = librosa.load(AUDIO_FILE_PATH, sr=16000)\n",
    "    print(\"Audio loaded and resampled to 16kHz successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading audio file. Make sure '{AUDIO_FILE_PATH}' is in the same directory.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 3. TASK 1: AUDIO-TO-TEXT (ASR) ---\n",
    "print(\"\\nLoading Whisper ASR model...\")\n",
    "\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-base\"\n",
    ")\n",
    "\n",
    "print(\"Transcribing audio... (This may take a moment)\")\n",
    "\n",
    "# --- THIS IS THE FIX ---\n",
    "# We add 'chunk_length_s=30' to tell the pipeline to\n",
    "# automatically chunk the long audio.\n",
    "transcribed_output = asr_pipeline(input_audio_array, chunk_length_s=30)\n",
    "transcribed_text = transcribed_output[\"text\"]\n",
    "\n",
    "print(\"\\n--- ✅ TRANSCRIBED TEXT ---\")\n",
    "print(transcribed_text)\n",
    "\n",
    "\n",
    "# --- 4. TASK 2: TEXT-TO-SUMMARY ---\n",
    "print(\"\\nLoading summarization model...\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "summary_output = summarizer(\n",
    "    transcribed_text, \n",
    "    max_length=150,\n",
    "    min_length=30, \n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(\"\\n--- ✅ FINAL SUMMARY (FROM AUDIO) ---\")\n",
    "print(summary_output[0]['summary_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convo_capsule",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
